{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97ee5d7",
   "metadata": {},
   "source": [
    "\n",
    "# Summary: Multilinear Regression with SVD and Confidence Intervals\n",
    "\n",
    "## 1. Singular Value Decomposition (SVD) for Regression\n",
    "\n",
    "Given a system $Ax = b$ where:\n",
    "- $A$: Design matrix (n_samples × n_features)\n",
    "- $b$: Target vector (n_samples × 1)\n",
    "- $x$: Coefficient vector to find (n_features × 1)\n",
    "\n",
    "### SVD Decomposition\n",
    "SVD factorizes $A$ into:\n",
    "$A = U \\Sigma V^T$\n",
    "where:\n",
    "- $U$: Left singular vectors (orthonormal basis for column space of A)\n",
    "- $\\Sigma$: Diagonal matrix of singular values ($\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_r > 0$)\n",
    "- $V$: Right singular vectors (orthonormal basis for row space of A)\n",
    "\n",
    "### Solving for x\n",
    "The least-squares solution is:\n",
    "$x = V \\Sigma^+ U^T b$\n",
    "where $\\Sigma^+$ is the pseudoinverse of $\\Sigma$ (reciprocal of non-zero $\\sigma_i$).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Key Concepts\n",
    "\n",
    "### Root Mean Square Error (RMSE)\n",
    "Measures model prediction error:\n",
    "$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (b_i - \\hat{b}_i)^2}$\n",
    "where $\\hat{b} = Ax$ are the predicted values.\n",
    "\n",
    "### Leverage ($h_i$)\n",
    "Quantifies how much influence a data point has on the model fit:\n",
    "$h_i = H_{ii} \\quad \\text{(diagonal of the hat matrix } H = U U^T)$\n",
    "- **Range**: $0 \\leq h_i \\leq 1$\n",
    "- **Interpretation**:\n",
    "  - $h_i \\approx 0$: Low influence\n",
    "  - $h_i \\approx 1$: High influence (outlier in feature space)\n",
    "\n",
    "### Standard Error of Prediction ($SE_{pred}$)\n",
    "Uncertainty for a prediction at point $A_i$:\n",
    "$SE_{\\text{pred}} = \\text{RMSE} \\times \\sqrt{1 + h_i}$\n",
    "- Accounts for both residual error (RMSE) and leverage.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Handling New Data Outside Training Set\n",
    "\n",
    "### Computing Leverage for New Data\n",
    "For a new observation $A_{\\text{new}}$ (1 × m vector):\n",
    "\n",
    "1. **Project onto SVD basis**:\n",
    "   $h_{\\text{new}} = A_{\\text{new}} (A^T A)^{-1} A_{\\text{new}}^T$\n",
    "\n",
    "2. **Using SVD components** (more stable):\n",
    "   $h_{\\text{new}} = \\|U^T A_{\\text{new}}^T\\|^2$\n",
    "\n",
    "### Python Implementation\n",
    "```python\n",
    "# For single new observation\n",
    "A_new = np.array([[...]])  # 1 × m vector\n",
    "h_new = (U.T @ A_new.T).ravel() @ (U.T @ A_new.T).ravel()\n",
    "\n",
    "# For multiple new observations (matrix A_new)\n",
    "h_new = np.sum((U.T @ A_new.T)**2, axis=0)\n",
    "```\n",
    "\n",
    "### Prediction Uncertainty\n",
    "- Standard Error:\n",
    "  $SE_{\\text{new}} = \\text{RMSE} \\times \\sqrt{1 + h_{\\text{new}}}$\n",
    "- 95% Prediction Interval:\n",
    "  $\\hat{b}_{\\text{new}} \\pm t_{n-m, 0.975} \\times SE_{\\text{new}}$\n",
    "\n",
    "### Interpretation\n",
    "- $h_{\\text{new}} \\approx 0$: Similar to training data → lower uncertainty\n",
    "- $h_{\\text{new}} \\gg 0$: Far from training data → higher uncertainty\n",
    "- When $h_{\\text{new}} > \\frac{2m}{n}$, the point is considered high-leverage\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Confidence Intervals (CI)\n",
    "\n",
    "### 95% CI for Mean Prediction\n",
    "$\\hat{b}_i \\pm t_{n-m, 0.975} \\cdot \\text{RMSE} \\sqrt{h_i}$\n",
    "where:\n",
    "- $t_{n-m, 0.975}$: Critical value from t-distribution with $n - m$ degrees of freedom\n",
    "- Covers the **true mean response** with 95% confidence.\n",
    "\n",
    "### 95% Prediction Interval (PI)\n",
    "For individual future observations:\n",
    "$\\hat{b}_i \\pm t_{n-m, 0.975} \\cdot \\text{RMSE} \\sqrt{1 + h_i}$\n",
    "- Wider than CI (accounts for both model uncertainty and noise).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Practical Steps\n",
    "\n",
    "1. **Compute SVD** of A:\n",
    "   ```python\n",
    "   U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "   ```\n",
    "\n",
    "2. **Calculate coefficients x**:\n",
    "   ```python\n",
    "   x = Vt.T @ np.diag(1/S) @ U.T @ b\n",
    "   ```\n",
    "\n",
    "3. **Compute leverage**:\n",
    "   ```python\n",
    "   h_i = np.diag(U @ U.T)\n",
    "   ```\n",
    "\n",
    "4. **RMSE**:\n",
    "   ```python\n",
    "   residuals = b - A @ x\n",
    "   RMSE = np.sqrt(np.mean(residuals**2))\n",
    "   ```\n",
    "\n",
    "5. **Confidence/Prediction Intervals**:\n",
    "   ```python\n",
    "   SE_pred = RMSE * np.sqrt(1 + h_i)  # For PI\n",
    "   CI = t.ppf(0.975, n - m) * RMSE * np.sqrt(h_i)  # For mean CI\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Interpretation\n",
    "\n",
    "- **High-leverage points** ($h_i \\to 1$) inflate $SE_{pred}$, reflecting higher uncertainty.\n",
    "- **New data warnings**:\n",
    "  - When $h_{\\text{new}} > \\frac{2m}{n}$, predictions are extrapolations\n",
    "  - Always report $h_{\\text{new}}$ alongside predictions\n",
    "- **95% CI**: \"Where the regression line is likely to be.\"\n",
    "- **95% PI**: \"Where future data points are likely to fall.\"\n",
    "\n",
    "This approach is numerically stable and handles rank-deficient matrices gracefully.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a6e47",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
